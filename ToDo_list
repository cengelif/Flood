"""
Decisions:
- Should we eliminate RTs: The default should be YES.
- Numbers are used as features
- if a clustering does not provide any candidate, change the thresholds in the next iteration!
- change token pattern of TfidfVectorizer! take one character features into account: I, a, ... : Elif did it.
- Should we eliminate tweets that contain only one normal word (.alpha())? It can be an option.: No, use other clues as well!
- For test tweets do the preprocessing via transform() method of tfidfvectorizer not fit_transform().
- n_clusters, for k should be assigned automatically at the beginning.


ToDo:
- silhouette score can be provided with an explanation.
- Write each group to files.
- while writing to the file: write remaining tweets as "rest".
- based on the identified/labeled tweets, a classifier may be able to predict label of a new cluster.
- support configuration files
- add create a classifier, test a classifier by classifying 10 docs and asking if they are good! option based on annotation after a while!
- tokenizer should process:  â€˜
- put an option to go out of the complete iteration. Currently q quits only from the current iteration.
- What should we do with the last batch of the clusters after we group majority of the tweets?
- Parallel processing for datasets that are bigger than n a certain number.
- Add possibility to getting online data as input. Think over processing online.
- If the data is too big, process/annotate a certain amount of it. Read the rest as you have less data in the memory. As you
  have labeled data, you can create a classifier from the labeled data and help the annotator.
- we need to have tests and proper sw package structure (check the project cookie clutter)
- how can you verify that the feature extraction step take almost all emoticons into account?
- implement implicit evaluation: same users/hashtags should be consistently under the same label.
- cluster users/hashtags based on users/hashtags
- include summary of a hashtag/user information in the cluster summary. Which information can be useful?
- Implement processing of text files as a method. Like read_tweets_from_text
- Be sure not to make any prediction for tweets that do not contain any feature from the training set. 
- Study pandas.
- Integrate tweet collecting to relevancer.
- Resolve conflict of column name difference between tokenized and untokenized text. Column name of the untokenized text is given as a default parameter. That will cause normalization to be done on untokenized text even at times we want to process tokenized text.
- Create timeline of the tweets in a cluster. You may need to include the datetime of the tweet.
- For more than 10.000 tweets you should use a server to run the script because of this, the matrix that occur in the get_and_eliminate_tweets method needs high memory so local memory is not enough.
- np.random() process time takes longtime. 

"""
