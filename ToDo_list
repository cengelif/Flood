"""
Decisions:
- Should we eliminate RTs: The default should be YES.
- Numbers are used as features
- if a clustering does not provide any candidate, change the thresholds in the next iteration!
- change token pattern of TfidfVectorizer! take one character features into account: I, a, ... : Elif did it.
- Should we eliminate tweets that contain only one normal word (.alpha())? It can be an option.: No, use other clues as well!
- For test tweets do the preprocessing via transform() method of tfidfvectorizer not fit_transform().
- n_clusters, for k should be assigned automatically at the beginning.


ToDo:
- silhouette score can be provided with an explanation.
- Write each group to files.
- while writing to the file: write remaining tweets as "rest".
- based on the identified/labeled tweets, a classifier may be able to predict label of a new cluster.
- support configuration files
- add create a classifier, test a classifier by classifying 10 docs and asking if they are good! option based on annotation after a while!
- tokenizer should process:  ‘
- put an option to go out of the complete iteration. Currently q quits only from the current iteration.
- What should we do with the last batch of the clusters after we group majority of the tweets?
- Parallel processing for datasets that are bigger than n a certain number.
- Add possibility to getting online data as input. Think over processing online.
- If the data is too big, process/annotate a certain amount of it. Read the rest as you have less data in the memory. As you
  have labeled data, you can create a classifier from the labeled data and help the annotator.
- we need to have tests and proper sw package structure (check the project cookie clutter)
- how can you verify that the feature extraction step take almost all emoticons into account?
- implement implicit evaluation: same users/hashtags should be consistently under the same label.
- cluster users/hashtags based on users/hashtags
- include summary of a hashtag/user information in the cluster summary. Which information can be useful?
- Implement processing of text files as a method. Like read_tweets_from_text
- Be sure not to make any prediction for tweets that do not contain any feature from the training set. 
- Study pandas.
- Integrate tweet collecting to relevancer.
- Resolve conflict of column name difference between tokenized and untokenized text. Column name of the untokenized text is given as a default parameter. That will cause normalization to be done on untokenized text even at times we want to process tokenized text.
- Create timeline of the tweets in a cluster. You may need to include the datetime of the tweet.
- For more than 10.000 tweets you should use a server to run the script because of this, the matrix that occur in the get_and_eliminate_tweets method needs high memory so local memory is not enough.
- np.random() process time takes longtime. 
- try not to use Global variables, check!
- Make Relevancer Object Oriented!
- Write test cases for each method.
- Use proper min() and max() methods when comparing the thresholds for closest and farthest distances from the cluster centers. Using first and last tweets after a sort is risky.

- Do more sophisticated cluster selection. If most of the tweets are close to the center, ignore the tail, use the
majority of the tweets. Use statistical tests for skewness. You can use %75 %25 quntiles.

- Do it stream based. A generator might be used. Make it available as soon as a cluster is found.

- Does feature extraction respect to new lines? Check. It should take the new lines into account.


EK:

Put an option to extend a topic. It can be a single class classifier, to find most similar tweets to that group.

Do a study on identifying distribution of the distances to the cluster center in the clusters. If first half of the distances focused a lot, ignore the second half. Check
first %25, second, etc. So the cluster selection can be optimized.

On the web interface put options for: Identify & Show retweets, Identify & Show Near-Duplicates, Upload tweets, Download group 

"dataframe info:None" satırına kontrol gerekiyor.

10 binlik datada "27-08-2015, 14:54,560 root INFO Number of features:56" satire wok az geldi gözüme. Kontrol etmeli. Ki kalan tweet sayısını da yazsak: "The data set is small enough to use Kmeans" civarına.

"In scikit-learn pairwise distance, we use different parameters for different distances that are defined in 'allowed_metrics'." şurada hang parametrelerin kabul edildiğini de yazsak, vey dökümantasyondaki nerve bakılabileceğine referans versek.

"End of the "scikit-learn pairwise distance"." Bunun sonuna hangi ölçütün kullanıldığını yazsak, tam olur.

Neden "number of features" hep, tüm tweet setlerde 56?

RT'ler ve URL'ler tam işlenmiyor gibime geldi. Şu featurelar şüpheli: 'co mLaTESfunR' --> url, 'RT @OliverMathenge' --> RT

User isimlerini de mi normalize etsek, "@cilginTweetci" gibilerini standard bi isme: "xxUserxx" gibi.. Yoksa çok kirlilik oluşturuyorlar sanki.


"""
