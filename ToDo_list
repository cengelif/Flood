"""


ToDo:
- Extract punctuation mark combinations that are used to express an emoticon. 

"Hi I am Ali :))" -> The token ":))" should be extracted as a feature, at least as ":)".

- Does bigram extraction ignore punctuation marks? In principle, a bigram should be created only if there is a space between two words, even \t and \n should not be allowed.
 "Hey, how are you doing? Are you OK?" -> Bigrams should not contain: "hey how" and "doing Are".

- We should include non-text based features. Length of a tweet should be taken into account. I will check some references about that.


- silhouette score can be provided with an explanation.
- Write each group to text files.
- while writing to the file: write remaining tweets as "rest".
- based on the identified/labeled tweets, a classifier may be able to predict label of a new cluster.
- support configuration files
- add create a classifier, test a classifier by classifying 10 docs and asking if they are good! option based on annotation after a while!
- tokenizer should process:  â€˜
- What should we do with the last batch of the clusters after we group majority of the tweets?
- Parallel processing for datasets that are bigger than a certain number.
- Add possibility to getting online data (Twitter, Twiqs.nl) as input. Think over processing online.
- If the data is too big, process/annotate a certain amount of it. Read the rest as you have less data in the memory. As you
  have labeled data, you can create a classifier from the labeled data and help the annotator.
- we need to have tests and proper sw package structure (check the project cookie clutter)
- how can you verify that the feature extraction step take almost all emoticons into account?
- implement implicit evaluation: same users/hashtags should be consistently under the same label.
- cluster users/hashtags based on users/hashtags
- include summary of a hashtag/user information in the cluster summary. Which information can be useful?
- Implement processing of text files as a method. Like read_tweets_from_text
- Be sure not to make any prediction for tweets that do not contain any feature from the training set. 
- Resolve conflict of column name difference between tokenized and untokenized text. Column name of the untokenized text is given as a default parameter. That will cause normalization to be done on untokenized text even at times we want to process tokenized text.
- Create timeline of the tweets in a cluster. You may need to include the datetime of the tweet.
- try not to use Global variables, check!
- Make Relevancer Object Oriented!
- Write test cases for each method.

- Do more sophisticated cluster selection. If most of the tweets are close to the center, ignore the tail, use the
majority of the tweets. Use statistical tests for skewness. You can use %75 %25 quntiles.

- Do it stream based. A generator might be used. Make it available as soon as a cluster is found.

- Does feature extraction respect to new lines? Check. It should take the new lines into account.

- Use generators to be able to process clusters as they are produced. A user should not need to wait to get all clusters.
- One cluster can be used as one class to identify other tweets that should be in that cluster from the remaining unclustered tweets. So a cluster will be bigger and annotation more efficient.
- Are we covering emoticons that contains punctuation marks? I do not think so :(( 
  We may be able to match any combination of punctuation marks after a basic normalization step, for example
  reducing repetitions of the same character etc.
 - Check what Remy is doing for normalization! Use the most obvious normalization steps. Be precise!


EK:

Put an option to extend a topic. It can be a single class classifier, to find most similar tweets to that group.

Do a study on identifying distribution of the distances to the cluster center in the clusters. If first half of the distances focused a lot, ignore the second half. Check
first %25, second, etc. So the cluster selection can be optimized.

On the web interface put options for: Identify & Show retweets, Identify & Show Near-Duplicates, Upload tweets, Download group 



Decisions:
- Should we eliminate RTs: The default should be YES.
- Numbers are used as features
- if a clustering does not provide any candidate, change the thresholds in the next iteration!
- change token pattern of TfidfVectorizer! take one character features into account: I, a, ... : Elif did it.
- Should we eliminate tweets that contain only one normal word (.alpha())? It can be an option.: No, use other clues as well!
- For test tweets do the preprocessing via transform() method of tfidfvectorizer not fit_transform().
- n_clusters, for k should be assigned automatically at the beginning.
- We normalize the user names and the urls to one token each.


"""
